# Rede Neural MLP para Classifica√ß√£o de Estrelas Pulsares

Este projeto apresenta a constru√ß√£o e o treinamento de uma Rede Neural **Multi-Layer Perceptron (MLP)** para a tarefa de classifica√ß√£o de estrelas pulsares. O grande diferencial desta implementa√ß√£o √© que a rede neural foi desenvolvida **"do zero"**, utilizando apenas a biblioteca NumPy, sem o aux√≠lio de frameworks de alto n√≠vel como TensorFlow ou Keras.

Al√©m disso, o projeto inclui um **Algoritmo Gen√©tico** para otimizar os hiperpar√¢metros da rede, buscando a configura√ß√£o que maximiza a performance do modelo.

## üéØ Objetivo do Projeto

O objetivo √© criar um classificador bin√°rio capaz de distinguir estrelas pulsares de outras fontes de ru√≠do c√≥smico com base em 8 features cont√≠nuas extra√≠das de medi√ß√µes astron√¥micas. O desafio secund√°rio √© demonstrar a constru√ß√£o dos mecanismos internos de uma rede neural e como t√©cnicas de meta-heur√≠stica podem ser aplicadas para sua otimiza√ß√£o.

## üõ†Ô∏è Tecnologias Utilizadas

* **Linguagem:** Python 3.x
* **Bibliotecas Principais:** NumPy, Pandas
* **Visualiza√ß√£o de Dados:** Matplotlib, Seaborn
* **Pr√©-processamento:** Scikit-learn (apenas para `MinMaxScaler` e `train_test_split`)
* **M√©tricas de Avalia√ß√£o:** Scikit-learn

## üìä An√°lise e Prepara√ß√£o dos Dados

O dataset utilizado foi o "Pulsar Star Prediction". A prepara√ß√£o dos dados consistiu nas seguintes etapas:

1.  **Leitura e Tratamento de Nulos:** Os dados foram carregados e os valores ausentes foram preenchidos utilizando a m√©dia da respectiva coluna.
    ```python
    dataset_train = pd.read_csv('pulsar_data_train.csv')
    dataset_test = pd.read_csv('pulsar_data_test.csv')
    dataset_train = dataset_train.fillna(dataset_train.mean())
    dataset_test = dataset_test.fillna(dataset_test.mean())
    ```

2.  **An√°lise de Correla√ß√£o:** Uma matriz de correla√ß√£o foi gerada para entender a rela√ß√£o entre as features.
    ![Matriz de Correla√ß√£o](matriz_correlacao.png)
    *(**Nota:** Salve a imagem gerada pelo seu c√≥digo como `matriz_correlacao.png` no reposit√≥rio)*

3.  **Divis√£o e Normaliza√ß√£o:** Os dados foram divididos em conjuntos de treino e teste, e as features foram normalizadas para a escala [0, 1] usando `MinMaxScaler` para garantir a estabilidade do treinamento da rede.
    ```python
    X = dataset_train.fillna(dataset_train.mean()).to_numpy()[:, 0:8]
    Y = dataset_train.fillna(dataset_train.mean()).to_numpy()[:, 8]

    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

    scaler = MinMaxScaler()
    X_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    ```

## üß† Rede Neural MLP "From Scratch"

A classe `MultiLayerPerceptron` foi implementada para encapsular toda a l√≥gica da rede, incluindo a inicializa√ß√£o de pesos, o feedforward, o c√°lculo da perda e o backpropagation.

### 1. Arquitetura
A rede possui a seguinte estrutura:
* Camada de Entrada (8 neur√¥nios, correspondentes √†s features)
* Camada Oculta 1 (n√∫mero de neur√¥nios vari√°vel)
* Fun√ß√£o de Ativa√ß√£o: Tangente Hiperb√≥lica (`tanh`)
* Camada Oculta 2 (n√∫mero de neur√¥nios vari√°vel)
* Fun√ß√£o de Ativa√ß√£o: Tangente Hiperb√≥lica (`tanh`)
* Camada de Sa√≠da (1 neur√¥nio)
* Fun√ß√£o de Ativa√ß√£o: Sigmoide (para classifica√ß√£o bin√°ria)

A inicializa√ß√£o dos pesos √© feita de forma a evitar o problema de "vanishing/exploding gradients", dividindo os valores aleat√≥rios pela raiz quadrada do n√∫mero de neur√¥nios da camada anterior.

### 2. Mecanismos
* **Feedforward (`forward`):** Propaga os dados de entrada atrav√©s das camadas, aplicando as fun√ß√µes de ativa√ß√£o, at√© gerar a predi√ß√£o na camada de sa√≠da.
    ```python
    def forward(self, x: np.ndarray) -> np.ndarray:
        self.z1 = x.dot(self.W1) + self.B1
        self.f1 = np.tanh(self.z1)

        self.z2 = self.f1.dot(self.W2) + self.B2
        self.f2 = np.tanh(self.z2)

        z3 = self.f2.dot(self.W3) + self.B3
        self.output = 1 / (1 + np.exp(-z3))
        return self.output
    ```
* **Backpropagation (`backpropagation`):** Calcula o gradiente do erro em rela√ß√£o a cada peso e bias, propagando-o da camada de sa√≠da para a de entrada, e atualiza os par√¢metros da rede usando o Gradiente Descendente.
    ```python
    def backpropagation(self, learning_rate: float) -> None:
        delta3 = self.output - self.y.reshape(-1, 1)
        dW3 = self.f2.T.dot(delta3) / self.x.shape[0]

        self.W1 -= learning_rate * dW1
    ```
* **Fun√ß√£o de Perda (`loss`):** Utiliza a Entropia Cruzada Bin√°ria, ideal para problemas de classifica√ß√£o bin√°ria.

## üß¨ Otimiza√ß√£o com Algoritmo Gen√©tico

Para encontrar a melhor combina√ß√£o de hiperpar√¢metros, um Algoritmo Gen√©tico foi implementado.
* **Cromossomo:** Cada indiv√≠duo da popula√ß√£o √© um "cromossomo" que representa uma configura√ß√£o da rede, contendo: `[√©pocas, taxa_de_aprendizagem, neuronios_oculta1, neuronios_oculta2]`.
* **Fun√ß√£o de Aptid√£o (Fitness):** A aptid√£o de cada cromossomo √© avaliada pelo **F1-Score** obtido pela MLP com seus respectivos hiperpar√¢metros no conjunto de teste. O F1-Score foi escolhido por ser uma m√©trica robusta para datasets desbalanceados.
* **Evolu√ß√£o:** O algoritmo evolui a popula√ß√£o por 20 gera√ß√µes, utilizando:
    * **Sele√ß√£o por Torneio:** Para escolher os pais.
    * **Crossover de Ponto √önico:** Para gerar descendentes.
    * **Muta√ß√£o:** Para introduzir diversidade e evitar m√≠nimos locais.

## üìà Resultados

Ap√≥s o treinamento da MLP com os hiperpar√¢metros otimizados, o modelo alcan√ßou as seguintes m√©tricas no conjunto de teste:

* **Acur√°cia:** 0.98
* **Precis√£o:** 0.92
* **Recall:** 0.71
* **F1-Score:** 0.80
* **AUC-ROC:** 0.85

A matriz de confus√£o abaixo ilustra o desempenho do classificador.

## ‚úíÔ∏è Autor

**[Kau√™ Santos]**

* [LinkedIn](https://www.linkedin.com/in/kau√™-santos-0a381b25a)
* [GitHub](https://github.com/KWSantos)